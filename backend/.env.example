# MProof Backend Configuration
# Copy this file to .env and adjust values as needed

# -----------------
# Ollama Configuration
# -----------------
# Ollama uses /api/chat endpoint, processes requests sequentially
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b
OLLAMA_TIMEOUT=60.0
OLLAMA_MAX_RETRIES=2
# Maximum tokens for response generation (Ollama handles larger contexts well)
OLLAMA_MAX_TOKENS=8192

# -----------------
# vLLM Configuration
# -----------------
# vLLM uses OpenAI-compatible /v1/chat/completions endpoint
# Can process multiple requests in parallel
VLLM_BASE_URL=http:/localhost:8000
VLLM_MODEL=llama3.2:3b
VLLM_TIMEOUT=60.0
VLLM_MAX_RETRIES=2
# Maximum tokens for response generation (lower for smaller context models)
VLLM_MAX_TOKENS=2048

# =============================================================================
# Database
# =============================================================================
DATABASE_URL=sqlite+aiosqlite:///./data/app.db

# =============================================================================
# Storage
# =============================================================================
DATA_DIR=./data

# =============================================================================
# OCR
# =============================================================================
TESSERACT_CONFIG=--psm 6
